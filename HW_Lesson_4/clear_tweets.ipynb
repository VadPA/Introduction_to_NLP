{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ac9f150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "import os\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dbe282da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52542, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "      <th>country</th>\n",
       "      <th>date_time</th>\n",
       "      <th>id</th>\n",
       "      <th>language</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>number_of_likes</th>\n",
       "      <th>number_of_shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>katyperry</td>\n",
       "      <td>Is history repeating itself...?#DONTNORMALIZEH...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12/01/2017 19:52</td>\n",
       "      <td>8.196330e+17</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7900</td>\n",
       "      <td>3472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>katyperry</td>\n",
       "      <td>@barackobama Thank you for your incredible gra...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/01/2017 08:38</td>\n",
       "      <td>8.191010e+17</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3689</td>\n",
       "      <td>1380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>katyperry</td>\n",
       "      <td>Life goals. https://t.co/XIn1qKMKQl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/01/2017 02:52</td>\n",
       "      <td>8.190140e+17</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10341</td>\n",
       "      <td>2387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>katyperry</td>\n",
       "      <td>Me right now üôèüèª https://t.co/gW55C1wrwd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/01/2017 02:44</td>\n",
       "      <td>8.190120e+17</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10774</td>\n",
       "      <td>2458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>katyperry</td>\n",
       "      <td>SISTERS ARE DOIN' IT FOR THEMSELVES! üôåüèªüí™üèª‚ù§Ô∏è ht...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10/01/2017 05:22</td>\n",
       "      <td>8.186890e+17</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17620</td>\n",
       "      <td>4655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>katyperry</td>\n",
       "      <td>happy 96th gma #fourmoreyears! üéà @ LACMA Los A...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>09/01/2017 01:00</td>\n",
       "      <td>8.182610e+17</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8242</td>\n",
       "      <td>2164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>katyperry</td>\n",
       "      <td>Kyoto, Japan \\r\\n1. 5. 17. https://t.co/o28M0v...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>06/01/2017 03:57</td>\n",
       "      <td>8.172180e+17</td>\n",
       "      <td>tl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7171</td>\n",
       "      <td>1910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>katyperry</td>\n",
       "      <td>üáØüáµ @ Sanrio Puroland https://t.co/eXVev5UMBx</td>\n",
       "      <td>NaN</td>\n",
       "      <td>02/01/2017 09:31</td>\n",
       "      <td>8.158530e+17</td>\n",
       "      <td>cy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7766</td>\n",
       "      <td>2468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>katyperry</td>\n",
       "      <td>2017 resolution: to embody authenticity!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>01/01/2017 03:31</td>\n",
       "      <td>8.154000e+17</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23662</td>\n",
       "      <td>8430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>katyperry</td>\n",
       "      <td>sisters. https://t.co/5ZE21x2aNk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27/12/2016 04:04</td>\n",
       "      <td>8.135960e+17</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9324</td>\n",
       "      <td>2637</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      author                                            content country  \\\n",
       "0  katyperry  Is history repeating itself...?#DONTNORMALIZEH...     NaN   \n",
       "1  katyperry  @barackobama Thank you for your incredible gra...     NaN   \n",
       "2  katyperry                Life goals. https://t.co/XIn1qKMKQl     NaN   \n",
       "3  katyperry            Me right now üôèüèª https://t.co/gW55C1wrwd     NaN   \n",
       "4  katyperry  SISTERS ARE DOIN' IT FOR THEMSELVES! üôåüèªüí™üèª‚ù§Ô∏è ht...     NaN   \n",
       "5  katyperry  happy 96th gma #fourmoreyears! üéà @ LACMA Los A...     NaN   \n",
       "6  katyperry  Kyoto, Japan \\r\\n1. 5. 17. https://t.co/o28M0v...     NaN   \n",
       "7  katyperry       üáØüáµ @ Sanrio Puroland https://t.co/eXVev5UMBx     NaN   \n",
       "8  katyperry           2017 resolution: to embody authenticity!     NaN   \n",
       "9  katyperry                   sisters. https://t.co/5ZE21x2aNk     NaN   \n",
       "\n",
       "          date_time            id language  latitude  longitude  \\\n",
       "0  12/01/2017 19:52  8.196330e+17       en       NaN        NaN   \n",
       "1  11/01/2017 08:38  8.191010e+17       en       NaN        NaN   \n",
       "2  11/01/2017 02:52  8.190140e+17       en       NaN        NaN   \n",
       "3  11/01/2017 02:44  8.190120e+17       en       NaN        NaN   \n",
       "4  10/01/2017 05:22  8.186890e+17       en       NaN        NaN   \n",
       "5  09/01/2017 01:00  8.182610e+17       en       NaN        NaN   \n",
       "6  06/01/2017 03:57  8.172180e+17       tl       NaN        NaN   \n",
       "7  02/01/2017 09:31  8.158530e+17       cy       NaN        NaN   \n",
       "8  01/01/2017 03:31  8.154000e+17       en       NaN        NaN   \n",
       "9  27/12/2016 04:04  8.135960e+17       en       NaN        NaN   \n",
       "\n",
       "   number_of_likes  number_of_shares  \n",
       "0             7900              3472  \n",
       "1             3689              1380  \n",
       "2            10341              2387  \n",
       "3            10774              2458  \n",
       "4            17620              4655  \n",
       "5             8242              2164  \n",
       "6             7171              1910  \n",
       "7             7766              2468  \n",
       "8            23662              8430  \n",
       "9             9324              2637  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.read_csv(\"_persistentId_persistentId\")\n",
    "print(tweets.shape)\n",
    "tweets.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3948d7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "apostrophe_dict = {\n",
    "\"ain't\": \"am not / are not\",\n",
    "\"aren't\": \"are not / am not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had / he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he shall / he will\",\n",
    "\"he'll've\": \"he shall have / he will have\",\n",
    "\"he's\": \"he has / he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how has / how is\",\n",
    "\"i'd\": \"I had / I would\",\n",
    "\"i'd've\": \"I would have\",\n",
    "\"i'll\": \"I shall / I will\",\n",
    "\"i'll've\": \"I shall have / I will have\",\n",
    "\"i'm\": \"I am\",\n",
    "\"i've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had / it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it shall / it will\",\n",
    "\"it'll've\": \"it shall have / it will have\",\n",
    "\"it's\": \"it has / it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had / she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she shall / she will\",\n",
    "\"she'll've\": \"she shall have / she will have\",\n",
    "\"she's\": \"she has / she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as / so is\",\n",
    "\"that'd\": \"that would / that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that has / that is\",\n",
    "\"there'd\": \"there had / there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there has / there is\",\n",
    "\"they'd\": \"they had / they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they shall / they will\",\n",
    "\"they'll've\": \"they shall have / they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had / we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what shall / what will\",\n",
    "\"what'll've\": \"what shall have / what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what has / what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when has / when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where has / where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who shall / who will\",\n",
    "\"who'll've\": \"who shall have / who will have\",\n",
    "\"who's\": \"who has / who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why has / why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had / you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you shall / you will\",\n",
    "\"you'll've\": \"you shall have / you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "short_word_dict = {\n",
    "\"121\": \"one to one\",\n",
    "\"a/s/l\": \"age, sex, location\",\n",
    "\"adn\": \"any day now\",\n",
    "\"afaik\": \"as far as I know\",\n",
    "\"afk\": \"away from keyboard\",\n",
    "\"aight\": \"alright\",\n",
    "\"alol\": \"actually laughing out loud\",\n",
    "\"b4\": \"before\",\n",
    "\"b4n\": \"bye for now\",\n",
    "\"bak\": \"back at the keyboard\",\n",
    "\"bf\": \"boyfriend\",\n",
    "\"bff\": \"best friends forever\",\n",
    "\"bfn\": \"bye for now\",\n",
    "\"bg\": \"big grin\",\n",
    "\"bta\": \"but then again\",\n",
    "\"btw\": \"by the way\",\n",
    "\"cid\": \"crying in disgrace\",\n",
    "\"cnp\": \"continued in my next post\",\n",
    "\"cp\": \"chat post\",\n",
    "\"cu\": \"see you\",\n",
    "\"cul\": \"see you later\",\n",
    "\"cul8r\": \"see you later\",\n",
    "\"cya\": \"bye\",\n",
    "\"cyo\": \"see you online\",\n",
    "\"dbau\": \"doing business as usual\",\n",
    "\"fud\": \"fear, uncertainty, and doubt\",\n",
    "\"fwiw\": \"for what it's worth\",\n",
    "\"fyi\": \"for your information\",\n",
    "\"g\": \"grin\",\n",
    "\"g2g\": \"got to go\",\n",
    "\"ga\": \"go ahead\",\n",
    "\"gal\": \"get a life\",\n",
    "\"gf\": \"girlfriend\",\n",
    "\"gfn\": \"gone for now\",\n",
    "\"gmbo\": \"giggling my butt off\",\n",
    "\"gmta\": \"great minds think alike\",\n",
    "\"h8\": \"hate\",\n",
    "\"hagn\": \"have a good night\",\n",
    "\"hdop\": \"help delete online predators\",\n",
    "\"hhis\": \"hanging head in shame\",\n",
    "\"iac\": \"in any case\",\n",
    "\"ianal\": \"I am not a lawyer\",\n",
    "\"ic\": \"I see\",\n",
    "\"idk\": \"I don't know\",\n",
    "\"imao\": \"in my arrogant opinion\",\n",
    "\"imnsho\": \"in my not so humble opinion\",\n",
    "\"imo\": \"in my opinion\",\n",
    "\"iow\": \"in other words\",\n",
    "\"ipn\": \"I‚Äôm posting naked\",\n",
    "\"irl\": \"in real life\",\n",
    "\"jk\": \"just kidding\",\n",
    "\"l8r\": \"later\",\n",
    "\"ld\": \"later, dude\",\n",
    "\"ldr\": \"long distance relationship\",\n",
    "\"llta\": \"lots and lots of thunderous applause\",\n",
    "\"lmao\": \"laugh my ass off\",\n",
    "\"lmirl\": \"let's meet in real life\",\n",
    "\"lol\": \"laugh out loud\",\n",
    "\"ltr\": \"longterm relationship\",\n",
    "\"lulab\": \"love you like a brother\",\n",
    "\"lulas\": \"love you like a sister\",\n",
    "\"luv\": \"love\",\n",
    "\"m/f\": \"male or female\",\n",
    "\"m8\": \"mate\",\n",
    "\"milf\": \"mother I would like to fuck\",\n",
    "\"oll\": \"online love\",\n",
    "\"omg\": \"oh my god\",\n",
    "\"otoh\": \"on the other hand\",\n",
    "\"pir\": \"parent in room\",\n",
    "\"ppl\": \"people\",\n",
    "\"r\": \"are\",\n",
    "\"rofl\": \"roll on the floor laughing\",\n",
    "\"rpg\": \"role playing games\",\n",
    "\"ru\": \"are you\",\n",
    "\"shid\": \"slaps head in disgust\",\n",
    "\"somy\": \"sick of me yet\",\n",
    "\"sot\": \"short of time\",\n",
    "\"thanx\": \"thanks\",\n",
    "\"thx\": \"thanks\",\n",
    "\"ttyl\": \"talk to you later\",\n",
    "\"u\": \"you\",\n",
    "\"ur\": \"you are\",\n",
    "\"uw\": \"you‚Äôre welcome\",\n",
    "\"wb\": \"welcome back\",\n",
    "\"wfm\": \"works for me\",\n",
    "\"wibni\": \"wouldn't it be nice if\",\n",
    "\"wtf\": \"what the fuck\",\n",
    "\"wtg\": \"way to go\",\n",
    "\"wtgp\": \"want to go private\",\n",
    "\"ym\": \"young man\",\n",
    "\"gr8\": \"great\"\n",
    "}\n",
    "\n",
    "emoticon_dict = {\n",
    "\":)\": \"happy\",\n",
    "\":‚Äë)\": \"happy\",\n",
    "\":-]\": \"happy\",\n",
    "\":-3\": \"happy\",\n",
    "\":->\": \"happy\",\n",
    "\"8-)\": \"happy\",\n",
    "\":-}\": \"happy\",\n",
    "\":o)\": \"happy\",\n",
    "\":c)\": \"happy\",\n",
    "\":^)\": \"happy\",\n",
    "\"=]\": \"happy\",\n",
    "\"=)\": \"happy\",\n",
    "\"<3\": \"happy\",\n",
    "\":-(\": \"sad\",\n",
    "\":(\": \"sad\",\n",
    "\":c\": \"sad\",\n",
    "\":<\": \"sad\",\n",
    "\":[\": \"sad\",\n",
    "\">:[\": \"sad\",\n",
    "\":{\": \"sad\",\n",
    "\">:(\": \"sad\",\n",
    "\":-c\": \"sad\",\n",
    "\":-< \": \"sad\",\n",
    "\":-[\": \"sad\",\n",
    "\":-||\": \"sad\"\n",
    "}\n",
    "\n",
    "fail_dict = {\n",
    "    \":)\": \":\\)\",\n",
    "    \":‚Äë)\": \":‚Äë\\)\",\n",
    "    \":-]\": \":-\\]\",\n",
    "    \"8-)\": \"8-\\)\",\n",
    "    \":o)\": \":o\\)\",\n",
    "    \":c)\": \":c\\)\",\n",
    "    \":^)\": \":\\^\\)\",\n",
    "    \"=)\": \"=\\)\",\n",
    "    \":-(\": \":-\\(\",\n",
    "    \":(\": \":\\(\",\n",
    "    \":[\": \":\\[\",\n",
    "    \">:[\": \">:\\[\",\n",
    "    \">:(\": \">:\\(\",\n",
    "    \":-[\": \":-\\[\",\n",
    "    \":-||\": \":-\\|\\|\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "346fb5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_txt(line):\n",
    "    spls = re.sub(r\"https?://[^,\\s]+,?\", \" \", line)\n",
    "    spls = re.sub(r'\\d+\\s?', \" \", spls).strip()\n",
    "    spls = re.sub(r'(\\b\\w+)\\s+\\1', r'\\1', spls)\n",
    "    spls = re.sub(r'[^–∞-—è–ê-–Ø—ë ]', ' ', spls)\n",
    "    spls = \" \".join(re.sub(html_chars, ' ', spls).split())\n",
    "    spls = \"\".join(i if i not in exclude else ' ' for i in spls.strip()).split()\n",
    "    spls = [morpher.parse(i.lower())[0].normal_form for i in spls]\n",
    "    spls = [i for i in spls if i not in sw and i != \"\"]\n",
    "    return spls\n",
    "\n",
    "def clean_name(name, text):\n",
    "    return str(re.sub(name, '', text))\n",
    "\n",
    "\n",
    "def change_of_dict(dictionary, text):\n",
    "    for el in text.split():\n",
    "        if el in dictionary:\n",
    "            if el in fail_dict:\n",
    "                el_temp = fail_dict[el]\n",
    "                text = re.sub(el_temp, dictionary[el], text)\n",
    "            else:\n",
    "                text = re.sub(el, dictionary[el], text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    '''\n",
    "    –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞\n",
    "\n",
    "    –Ω–∞ –≤—ã—Ö–æ–¥–µ –æ—á–∏—â–µ–Ω—ã–π —Ç–µ–∫—Å—Ç\n",
    "    '''\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "\n",
    "    vect_func = np.vectorize(clean_name)\n",
    "    \n",
    "    text = html.unescape(text)              # 1. –ó–∞–º–µ–Ω–∏–º html-—Å—É—â–Ω–æ—Å—Ç–∏\n",
    "    list_name = re.findall(\"@[\\w]*\", text)  # 2. –£–¥–∞–ª–∏–º @user –∏–∑ –≤—Å–µ—Ö —Ç–≤–∏—Ç–æ–≤\n",
    "\n",
    "    if list_name:\n",
    "        text = vect_func(list_name, text)\n",
    "        text = str(text[0])\n",
    "\n",
    "    text = text.lower()                     # 3. –ò–∑–º–µ–Ω–∏–º —Ä–µ–≥–∏—Å—Ç—Ä —Ç–≤–∏—Ç–æ–≤ –Ω–∞ –Ω–∏–∂–Ω–∏–π —Å –ø–æ–º–æ—â—å—é .lower().\n",
    "    \n",
    "    text = re.sub(r\"https?://[^,\\s]+,?\", \" \", text)\n",
    "\n",
    "    # 4. –ó–∞–º–µ–Ω–∏–º —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è —Å –∞–ø–æ—Å—Ç—Ä–æ—Ñ–∞–º–∏ –Ω–∞ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ–ª–Ω—É—é –≤–µ—Ä—Å–∏—é —Å–ª–æ–≤–∞\n",
    "    text = change_of_dict(apostrophe_dict, text)\n",
    "\n",
    "    # 5. –ó–∞–º–µ–Ω–∏–º —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è –Ω–∞ –∏—Ö –ø–æ–ª–Ω—ã–µ —Ñ–æ—Ä–º—ã\n",
    "    text = change_of_dict(short_word_dict, text)\n",
    "\n",
    "    # 6. –ó–∞–º–µ–Ω–∏–º —ç–º–æ—Ç–∏–∫–æ–Ω—ã\n",
    "    text = change_of_dict(emoticon_dict, text)\n",
    "\n",
    "    # 7. –ó–∞–º–µ–Ω–∏–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –Ω–∞ –ø—Ä–æ–±–µ–ª—ã\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "\n",
    "    # 8. –ó–∞–º–µ–Ω–∏–º —Å–ø–µ—Ü. —Å–∏–º–≤–æ–ª—ã –Ω–∞ –ø—Ä–æ–±–µ–ª—ã\n",
    "    text = re.sub(r'[^a-zA-Z0-9]', ' ', text)\n",
    "\n",
    "    # 9. –ó–∞–º–µ–Ω–∏–º —á–∏—Å–ª–∞ –Ω–∞ –ø—Ä–æ–±–µ–ª—ã\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "\n",
    "    # 10. –£–¥–∞–ª–∏–º –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å–ª–æ–≤–∞ –¥–ª–∏–Ω–æ–π –≤ 1 —Å–∏–º–≤–æ–ª\n",
    "    text = ' '.join([w for w in text.split() if len(w) > 1])\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4ce87018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 1e+03 ¬µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import html\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9fc0e2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 52542/52542 [00:07<00:00, 6711.20it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>is history repeating itself dontnormalizehate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>thank you for your incredible grace in leaders...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>life goals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>me right now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sisters are doin it for themselves</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet\n",
       "0      is history repeating itself dontnormalizehate\n",
       "1  thank you for your incredible grace in leaders...\n",
       "2                                         life goals\n",
       "3                                       me right now\n",
       "4                 sisters are doin it for themselves"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –ó–∞–ø—É—Å–∫–∞–µ–º –æ—á–∏—Å—Ç–∫—É —Ç–µ–∫—Å—Ç–∞.\n",
    "df_tweets = pd.DataFrame(tweets['content'].progress_apply(lambda x: clean_text(x)), columns=['content'])\n",
    "df_tweets.columns = ['tweet']\n",
    "df_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "442c805d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\–í–∞–¥–∏–º\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "from nltk import tokenize as tknz\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c4a93b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 52542/52542 [00:12<00:00, 4324.80it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0        [is, history, repeating, itself, dontnormalize...\n",
       "1        [thank, you, for, your, incredible, grace, in,...\n",
       "2                                            [life, goals]\n",
       "3                                         [me, right, now]\n",
       "4                [sisters, are, doin, it, for, themselves]\n",
       "                               ...                        \n",
       "52537           [life, could, not, be, better, right, now]\n",
       "52538    [first, monday, back, in, action, had, would, ...\n",
       "52539    [crime, shows, buddy, snuggles, the, perfect, ...\n",
       "52540                                                   []\n",
       "52541                                                   []\n",
       "Name: tweet_token, Length: 52542, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets['tweet_token'] = df_tweets['tweet'].progress_apply(lambda x: tknz.word_tokenize(x))\n",
    "df_tweets.tweet_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "16c49654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c68f243a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "98d0b2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_stop_words(words, stop_words):\n",
    "    return [word for word in words if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "053d93a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 52542/52542 [00:00<00:00, 199781.09it/s]\n"
     ]
    }
   ],
   "source": [
    "df_tweets['tweet_token_filtered'] = df_tweets['tweet_token'].progress_apply(lambda x: del_stop_words(x, stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dfe1f509",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "48c3857e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a284f150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def used_stemmer(stemmer, words):\n",
    "    return [stemmer.stem(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0c5fe0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 52542/52542 [00:14<00:00, 3590.65it/s]\n"
     ]
    }
   ],
   "source": [
    "df_tweets['tweet_stemmed'] = df_tweets['tweet_token_filtered'].progress_apply(lambda x: used_stemmer(stemmer, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cfba410a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                        [histori, repeat, dontnormalizeh]\n",
       "1               [thank, incred, grace, leadership, except]\n",
       "2                                             [life, goal]\n",
       "3                                                  [right]\n",
       "4                                           [sister, doin]\n",
       "                               ...                        \n",
       "52537                         [life, could, better, right]\n",
       "52538    [first, monday, back, action, would, say, mile...\n",
       "52539    [crime, show, buddi, snuggl, perfect, sunday, ...\n",
       "52540                                                   []\n",
       "52541                                                   []\n",
       "Name: tweet_stemmed, Length: 52542, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets.tweet_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f721f55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def used_lemmatizer(lemmatizer, words):\n",
    "    return [lemmatizer.lemmatize(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eb8c64b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 52542/52542 [00:06<00:00, 8069.72it/s]\n"
     ]
    }
   ],
   "source": [
    "df_tweets['tweet_lemmatized'] = df_tweets['tweet_token_filtered'].progress_apply(lambda x: used_lemmatizer(lemmatizer, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eb66c8d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                  [history, repeating, dontnormalizehate]\n",
       "1        [thank, incredible, grace, leadership, excepti...\n",
       "2                                             [life, goal]\n",
       "3                                                  [right]\n",
       "4                                           [sister, doin]\n",
       "                               ...                        \n",
       "52537                         [life, could, better, right]\n",
       "52538    [first, monday, back, action, would, say, mile...\n",
       "52539    [crime, show, buddy, snuggle, perfect, sunday,...\n",
       "52540                                                   []\n",
       "52541                                                   []\n",
       "Name: tweet_lemmatized, Length: 52542, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets.tweet_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "858b93b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets['tweet_stemmed'] = df_tweets['tweet_stemmed'].apply(lambda x: ' '.join(x))\n",
    "df_tweets['tweet_lemmatized'] = df_tweets['tweet_lemmatized'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f5c814fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(r'df_dop_tweet.txt', 'wb')\n",
    "pickle.dump(df_tweets, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "121d3325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_token</th>\n",
       "      <th>tweet_token_filtered</th>\n",
       "      <th>tweet_stemmed</th>\n",
       "      <th>tweet_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>is history repeating itself dontnormalizehate</td>\n",
       "      <td>[is, history, repeating, itself, dontnormalize...</td>\n",
       "      <td>[history, repeating, dontnormalizehate]</td>\n",
       "      <td>histori repeat dontnormalizeh</td>\n",
       "      <td>history repeating dontnormalizehate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>thank you for your incredible grace in leaders...</td>\n",
       "      <td>[thank, you, for, your, incredible, grace, in,...</td>\n",
       "      <td>[thank, incredible, grace, leadership, excepti...</td>\n",
       "      <td>thank incred grace leadership except</td>\n",
       "      <td>thank incredible grace leadership exceptional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>life goals</td>\n",
       "      <td>[life, goals]</td>\n",
       "      <td>[life, goals]</td>\n",
       "      <td>life goal</td>\n",
       "      <td>life goal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>me right now</td>\n",
       "      <td>[me, right, now]</td>\n",
       "      <td>[right]</td>\n",
       "      <td>right</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sisters are doin it for themselves</td>\n",
       "      <td>[sisters, are, doin, it, for, themselves]</td>\n",
       "      <td>[sisters, doin]</td>\n",
       "      <td>sister doin</td>\n",
       "      <td>sister doin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  \\\n",
       "0      is history repeating itself dontnormalizehate   \n",
       "1  thank you for your incredible grace in leaders...   \n",
       "2                                         life goals   \n",
       "3                                       me right now   \n",
       "4                 sisters are doin it for themselves   \n",
       "\n",
       "                                         tweet_token  \\\n",
       "0  [is, history, repeating, itself, dontnormalize...   \n",
       "1  [thank, you, for, your, incredible, grace, in,...   \n",
       "2                                      [life, goals]   \n",
       "3                                   [me, right, now]   \n",
       "4          [sisters, are, doin, it, for, themselves]   \n",
       "\n",
       "                                tweet_token_filtered  \\\n",
       "0            [history, repeating, dontnormalizehate]   \n",
       "1  [thank, incredible, grace, leadership, excepti...   \n",
       "2                                      [life, goals]   \n",
       "3                                            [right]   \n",
       "4                                    [sisters, doin]   \n",
       "\n",
       "                          tweet_stemmed  \\\n",
       "0         histori repeat dontnormalizeh   \n",
       "1  thank incred grace leadership except   \n",
       "2                             life goal   \n",
       "3                                 right   \n",
       "4                           sister doin   \n",
       "\n",
       "                                tweet_lemmatized  \n",
       "0            history repeating dontnormalizehate  \n",
       "1  thank incredible grace leadership exceptional  \n",
       "2                                      life goal  \n",
       "3                                          right  \n",
       "4                                    sister doin  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open(r'df_dop_tweet.txt', 'rb')\n",
    "df_tweets_load = pickle.load(f)\n",
    "f.close()\n",
    "df_tweets_load.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadc294b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
