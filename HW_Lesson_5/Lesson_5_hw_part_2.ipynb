{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6272899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.tag import DefaultTagger\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.tag import BigramTagger, TrigramTagger\n",
    "\n",
    "import pyconll\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1c7c23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train = pyconll.load_from_file('ru_syntagrus-ud-train.conllu')\n",
    "full_test = pyconll.load_from_file('ru_syntagrus-ud-dev.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b610c324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Снова ADV\n",
      "приобрел VERB\n",
      "дозу NOUN\n",
      ", PUNCT\n",
      "\n",
      "В ADP\n",
      "женщине NOUN\n",
      "важна ADJ\n",
      "верность NOUN\n",
      ", PUNCT\n",
      "а CCONJ\n",
      "не PART\n",
      "красота NOUN\n",
      ". PUNCT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent in full_train[:2]:\n",
    "    for token in sent:\n",
    "        print(token.form, token.upos)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ea80546",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdata_train = []\n",
    "for sent in full_train[:]:\n",
    "    fdata_train.append([(token.form, token.upos) for token in sent])\n",
    "    \n",
    "fdata_test = []\n",
    "for sent in full_test[:]:\n",
    "    fdata_test.append([(token.form, token.upos) for token in sent])\n",
    "    \n",
    "fdata_sent_test = []\n",
    "for sent in full_test[:]:\n",
    "    fdata_sent_test.append([token.form for token in sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "501c3fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наибольшая длина предложения 274\n",
      "Наибольшая длина токена 161\n"
     ]
    }
   ],
   "source": [
    "MAX_SENT_LEN = max(len(sent) for sent in full_train)\n",
    "MAX_ORIG_TOKEN_LEN = max(len(token.form) for sent in full_train for token in sent)\n",
    "print('Наибольшая длина предложения', MAX_SENT_LEN)\n",
    "print('Наибольшая длина токена', MAX_ORIG_TOKEN_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a42b84b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Снова приобрел дозу ,\n",
      "В женщине важна верность , а не красота .\n",
      "Важно то , о чем ты думаешь и какие поступки совершаешь .\n",
      "Уже не та на лоб спадает челка ...\n",
      "Но ты не живешь по-евангельски , и это — причина твоих проблем .\n",
      "Ведь этот цветок цветёт для меня !\n",
      "И как же больно было нам ,\n",
      "Как свет добра струился с глаз !\n",
      "Их отличает харизма , приятная внешность , живой аналитический ум , хорошее воспитание и манеры , за которыми всегда спрятан сильный взгляд , в котором едва просматривается небольшая грусть .\n",
      "Увы , не понимаю ... Где эта жизнь , которая моя .\n"
     ]
    }
   ],
   "source": [
    "all_train_texts = [' '.join(token.form for token in sent) for sent in full_train]\n",
    "all_test_texts = [' '.join(token.form for token in sent) for sent in full_test]\n",
    "\n",
    "all_train_labels = [' '.join(token.form for token in sent) for sent in full_train]\n",
    "all_test_labels = [' '.join(token.form for token in sent) for sent in full_test]\n",
    "print('\\n'.join(all_train_texts[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3782767",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_tagger = BigramTagger(fdata_train, backoff=UnigramTagger(fdata_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1462107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Не', 'PART'), ('могу', 'VERB'), ('найти', 'VERB'), ('воробья', None)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.6859152139461173"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(bigram_tagger.tag(fdata_sent_test[100]), bigram_tagger.evaluate(fdata_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd345975",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tok = []\n",
    "train_label = []\n",
    "for sent in fdata_train[:]:\n",
    "    for tok in sent:\n",
    "        train_tok.append(tok[0])\n",
    "        train_label.append('NO_TAG' if tok[1] is None else tok[1])\n",
    "        \n",
    "test_tok = []\n",
    "test_label = []\n",
    "for sent in fdata_test[:]:\n",
    "    for tok in sent:\n",
    "        test_tok.append(tok[0])\n",
    "        test_label.append('NO_TAG' if tok[1] is None else tok[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "600599e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "#import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cabd3070",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "train_enc_labels = le.fit_transform(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b84dc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_enc_labels = le.transform(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c129f472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM',\n",
       "       'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X'],\n",
       "      dtype='<U5')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab3e333c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hvectorizer = HashingVectorizer(ngram_range=(1, 3), analyzer='char', n_features=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "715fe4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = hvectorizer.fit_transform(train_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cf9fd77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = hvectorizer.transform(test_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f1262e98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(176631, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dad2c0a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000, random_state=0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(random_state=0, max_iter=1000)\n",
    "lr.fit(X_train, train_enc_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f6dd85a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "488af1de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.731378763866878"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_enc_labels, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "06a9c3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hvectorizer = HashingVectorizer(ngram_range=(1, 5), analyzer='char', n_features=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1e771380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(176631, 300)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = hvectorizer.fit_transform(train_tok)\n",
    "X_test = hvectorizer.transform(test_tok)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8341b828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000, random_state=0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(random_state=0, max_iter=1000)\n",
    "lr.fit(X_train, train_enc_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e0f8047e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "798c3051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7138470681458003"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_enc_labels, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "69bd96c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8820324881141046"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hvectorizer = CountVectorizer(ngram_range=(1, 5), analyzer='char')\n",
    "X_train = hvectorizer.fit_transform(train_tok)\n",
    "X_test = hvectorizer.transform(test_tok)\n",
    "lr.fit(X_train, train_enc_labels)\n",
    "pred = lr.predict(X_test)\n",
    "accuracy_score(test_enc_labels, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "266341d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "J:\\Vadim\\envs\\TensorFlowEd\\lib\\site-packages\\xgboost\\compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0f7048b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "J:\\Vadim\\envs\\TensorFlowEd\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:15:38] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8974841521394612"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hvectorizer = CountVectorizer(ngram_range=(1, 5), analyzer='char')\n",
    "X_train = hvectorizer.fit_transform(train_tok)\n",
    "X_test = hvectorizer.transform(test_tok)\n",
    "xgb_cl = XGBClassifier(learning_rate=0.1, max_depth=28, n_estimators=150, random_state=0)\n",
    "xgb_cl.fit(X_train, train_enc_labels)\n",
    "pred = xgb_cl.predict(X_test)\n",
    "accuracy_score(test_enc_labels, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77283d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
